<div class="m-4 space-y-4">


    

    <div>
        <mat-card class="example-card text-2xl" appearance="outlined">

            <mat-card-content class="text-lg">
                <h3 class="text-2xl font-semibold font-sans " style="color: blue;">
                    12.8 Prediction with Quantizer
                </h3>


                <p class="mt-4">
                    The block diagram of a predictive encoder with quantizer can be seen in Fig. (12.18).
                    Here we can see a predictive encoder with quantization of the prediction error. In order
                    to make sure the encoder predictor works on the quantized values, like in the decoder,
                    it uses a decoder in the encoder structure, which produces the quantized reconstructed
                    value Q(x(n))(the function Q() here include both, quantizer and de-quantizer).


                </p>
                <p class="mt-4">
                    The decoder stays the same, except for the de-quantization of the prediction error in
                    the beginning, as shown in Fig. (12.19).
                    Observe: The reconstructed signal x(n) is the same as for the encoder, plus the quantization error
                    from the quantizer:
                </p>
                <p class="text-lg font-mono mt-3 text-center" appMathJax="$$ 
Q(x(n)) = e(n) + \hat{x}(n)
 $$"></p>

            </mat-card-content>

        </mat-card>
    </div>

    <div>
        <mat-card class="example-card text-2xl" appearance="outlined">

            <mat-card-content class="text-lg">
                <h3 class="text-2xl font-semibold font-sans " style="color: blue;">
                    12.8.1 LMS with Quantizer Python Example
                </h3>


                <p class="mt-4">
                    Write a Python program file with
                </p>
                <pre class="bg-gray-100 text-sm p-3 rounded-lg overflow-x-auto font-mono mt-2">

import numpy as np
from sound import *
import matplotlib.pyplot as plt

x, fs = wavread("fspeech.wav")

# normalized float, -1 < x < 1
x = np.array(x, dtype=float) / 2**15

print(np.size(x))

e = np.zeros(np.size(x))
xrek = np.zeros(np.size(x))
P = 0
L = 10

h = np.zeros(L)

# start values same as decoder
x[0:L] = 0.0

quantstep = 0.01

for n in range(L, len(x)):

    if n > 4000 and n < 4010:
        print("encoder h:", h, "e =", e[n])

    # predicted value from past reconstructed values
    P = np.dot(np.flipud(xrek[n - L + np.arange(L)]), h)

    # quantize and dequantize prediction error
    e[n] = np.round((x[n] - P) / quantstep) * quantstep

    # decoder inside encoder: reconstructed value
    xrek[n] = e[n] + P

    # LMS update rule
    h = h + 1.0 * e[n] * np.flipud(xrek[n - L + np.arange(L)])

# error statistics
print("Mean squared prediction error:", np.dot(e, e) / np.max(np.size(e)))
print("Mean squared signal power:", np.dot(x.T, x) / np.max(np.size(x)))
print("Signal to Error ratio:", np.dot(x.T, x) / np.dot(e.T, e))

# listen to the error
sound(2**15 * e, fs)

plt.figure()
plt.plot(x)
plt.plot(e, "r")
plt.xlabel("Sample")
plt.ylabel("Normalized Sample")
plt.title("Least Mean Squares (LMS) Online Adaptation")
plt.legend(("Original", "Prediction Error"))
plt.show()

# Decoder
h = np.zeros(L)
xrek = np.zeros(np.size(x))

for n in range(L, len(x)):

    if n > 4000 and n < 4010:
        print("decoder h:", h)

    P = np.dot(np.flipud(xrek[n - L + np.arange(L)]), h)
    xrek[n] = e[n] + P

    # LMS update
    h = h + 1.0 * e[n] * np.flipud(xrek[n - L + np.arange(L)])

plt.plot(xrek)
plt.xlabel("Sample")
plt.ylabel("Normalized Sample")
plt.title("The Reconstructed Signal")
plt.show()

# listen to reconstructed signal
sound(2**15 * xrek, fs)


</pre>
                <p>
                    Execute it with
                    python lmsquantexample.py
                    Observe: Because of the quantization, the prediction error now clearly increased.
                    Observe: The signal is now fully decoded, even with quantization, although with a
                    little noise, which was to be expected. But we can avoid the noise by reducing the
                    quantization step size.
                    Observe that this structure for the decoder in the encoder also applies to the other
                    prediction methods
                </p>

                <p class="text-center mt-3 text-lg font-mono">
                    formula
                </p>
            </mat-card-content>

        </mat-card>
    </div>
</div>